# Issue: Node down => Prometheus server can't access node-exporter metrics

## Summary

**ë…¸ë“œëŠ” ëª¨ë‘ ì •ìƒ ì‘ë™ì¤‘ì´ì§€ë§Œ, 1. prometheus alert ruleì— ì˜í•´ ë‹¤ìš´ë˜ì–´ìˆë‹¤ê³  íŒë³„ 2. ê·¸ë¼íŒŒë‚˜ ëŒ€ì‹œë³´ë“œë¡œ ë§ˆìŠ¤í„° ë…¸ë“œë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë…¸ë“œë“¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ì—†ìŒ**

íŠ¹ì´ì‚¬í•­: ì´ì „ ë°°í¬ì—ì„œëŠ” ë¬¸ì œ ëœ ì ì´ ì—†ì—ˆë‹¤. ë‹¤ë¥¸ ìƒí™©ì´ë¼ê³ ëŠ” ë‹¤ì¤‘ê°€ìš©ì˜ì—­ì—ì„œ ì²˜ìŒ ë°°í¬ë¼ëŠ” ê²ƒ. í•˜ì§€ë§Œ, ì •ìƒ ì‘ë™í•˜ëŠ” master nodeì™€ ê°™ì€ zoneì— ìˆëŠ” worker1ë„ ìŠ¤í¬ë© í•  ìˆ˜ ì—†ëŠ” ìƒí™©ì„ ê³ ë ¤í•˜ë©´ ë‹¤ì¤‘ ê°€ìš©ì˜ì—­ì´ë¼ëŠ” ì ì´ ë¬¸ì œì¼ê¹Œ ì‹¶ë‹¤.

<img width="1433" alt="image" src="https://user-images.githubusercontent.com/28949162/204507614-f9bf5145-9c56-4e89-8f6e-77bbc1cff6fb.png">

ì¸ìŠ¤í„´ìŠ¤ê°€ ëª¨ë‘ ì‘ë™ì¤‘ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  node-exporterê°€ ë‹¤ìš´ ë˜ìˆë‹¤.

<img width="840" alt="image" src="https://user-images.githubusercontent.com/28949162/204508168-7d8254ad-7447-47f5-b8e6-31300028ef60.png">

pod status == Ready ìƒíƒœì´ì§€ë§Œ, 1. ê·¸ë¼íŒŒë‚˜ ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ê³ , 2. í”„ë¡œë©”í…Œìš°ìŠ¤ ì„œë²„ì—ì„œ ë…¸ë“œ ì •ë³´ê°€ ì¡°íšŒ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ë§ˆìŠ¤í„° node-exporterë¥¼ ì œì™¸í•œ worker ë…¸ë“œë“¤ ì „ë¶€ node-exporterê°€ ì •ìƒ ì‘ë™í•˜ì§€ ì•ŠëŠ”ë‹¤.

```
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  11s   default-scheduler  0/4 nodes are available: 1 node(s) didn't have free ports for the requested pod ports, 3 node(s) didn't match Pod's node affinity/selector.
  Warning  FailedScheduling  10s   default-scheduler  0/4 nodes are available: 1 node(s) didn't have free ports for the requested pod ports, 3 node(s) didn't match Pod's node affinity/selector.
  Normal   Scheduled         8s    default-scheduler  Successfully assigned default/prometheus-prometheus-node-exporter-544xt to ip-10-0-3-139.ap-northeast-2.compute.internal
```

Node exporter ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì»´í¬ë„ŒíŠ¸ëŠ” ì œëŒ€ë¡œ ì‘ë™í•œë‹¤ ì–´ë–¤ê²Œ ë¬¸ì œì¼ê¹Œ?

curl localhost:9100/metrics ë¥¼ ê° ë…¸ë“œì—ì„œ í•˜ë©´ ë©”íŠ¸ë¦­ì„ ê²Œì‹œí•˜ëŠ”ì§€ëŠ” ì•Œ ìˆ˜ ìˆë‹¤. -> ê° ë…¸ë“œì—ì„œ ê°€ëŠ¥

https://ooeunz.tistory.com/139 ì—¬ê¸°ì— ë‹¤ë¥´ë©´

1. í”„ë¡œë©”í…Œìš°ìŠ¤ ì„œë²„ëŠ” ìµìŠ¤í¬í„°ê°€ ì—´ì–´ë‘” http endpoint ì— ì ‘ì†í•˜ì—¬ exporter ê°€ ìˆ˜ì§‘í•œ metricì„ ìˆ˜ì§‘í•˜ê³  í”„ë¡œë©”í…Œìš°ìŠ¤ ì„œë²„ì— ì €ì¥í•œë‹¤.
2. í”„ë¡œë©”í…Œìš°ìŠ¤ ì„œë²„ê°€ HTTP endpointì— ì ‘ê·¼í•˜ì—¬ ëª¨ë‹ˆí„°ë§ ëŒ€ìƒì˜ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•´ì˜¤ë„ë¡ scrape configì— metric scrape job ì„ ë“±ë¡í•  ìˆ˜ ìˆë‹¤. ì´ë•Œ ë“±ë¡ëœ job ì€ target url ì— ì—°ê²° ëœ instance ë“¤ì—ê²Œì„œ ì£¼ê¸°ì ìœ¼ë¡œ metricì„ ìˆ˜ì§‘í•´ ì˜¨ë‹¤.

ì´ë¥¼ ìƒê°í•´ ë´¤ì„ ë–„, ì²« ë²ˆì§¸ë¡œ ì˜ì‹¬ê°€ëŠ” ì›ì¸ì€,

1. í”„ë¡œë©”í…Œìš°ìŠ¤ ì„œë²„ê°€ ìµìŠ¤í¬í„°ê°€ ì—´ì–´ë‘” http endpointì— ì ‘ì†ì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸.

![image](https://user-images.githubusercontent.com/28949162/204684922-32ca2775-99fa-428c-9bad-5913081b40cf.png)

kubernetes-service-endpoint(job) ê°€ ë…¸ë“œ ìµìŠ¤í¬í„°ì— ì ‘ê·¼í•˜ì§€ ëª»í•˜ëŠ” ê²ƒì¼ ìˆ˜ ìˆë‹¤.

kubernetes-service-endpoint ë¥¼ describe í•˜ë ¤ê³  í•´ë„ ì‹¤í–‰ì¤‘ì¸ ì¡ì„ ë³¼ ìˆ˜ ì—†ë‹¤. ì¡ì€ ì‹¤í–‰ë˜ê³  ì—­í• ì„ ëë‚´ë©´ ì¢…ë£Œí•œë‹¤.


![image](https://user-images.githubusercontent.com/28949162/204696143-a121b424-0abb-4f88-ac40-82c9975deabc.png)


![image](https://user-images.githubusercontent.com/28949162/204699142-9c1f8820-ea6c-4965-9393-7ed7e9eab00e.png)

ìœ„ ì‚¬ì§„ê³¼ ê°™ì´ ë‹¤ë¥¸ master nodeì—ì„œ ë‹¤ë¥¸ worker nodeì˜ ë©”íŠ¸ë¦­ì´ ì ‘ê·¼ ë¶ˆê°€ëŠ¥í•˜ë‹¤.

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„, í”„ë¡œë©”í…Œìš°ìŠ¤ ì„œë²„ê°€ endpointì— ì ‘ê·¼í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ê°€ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ ì•Œì•„ë³´ì.

í”„ë¡œë©”í…Œìš°ìŠ¤ëŠ” SDë¥¼ í†µí•´ targetì„ ê°€ì ¸ì˜¨ë‹¤. ì´ë•Œ, targetì— ëŒ€í•œ ì •ë³´ëŠ” configMap ì— ìˆë‹¤.

`k get configmap prometheus-server -o yaml > config_server.yaml` # configmap í™•ì¸

ì•„ë˜ëŠ” configMap ì¤‘ job_name: kubernetes-service-endpoints ì— ëŒ€í•œ ë¶€ë¶„ë§Œ ë°œì·Œí–ˆë‹¤.

```yaml
     job_name: kubernetes-service-endpoints
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape
      - action: drop
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: (.+?)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: service
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
    - honor_labels: true
```

ë´ë„ ë¬´ìŠ¨ëœ»ì¸ì§€ ì „í˜€ ëª¨ë¥´ê² ë‹¤! 

ë°‘ì„ ë³´ë©´ ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •ì€ ì•Œë§ê²Œ ë˜ì–´ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
```
 âš¡ root@ip-10-0-3-139 î‚° ~/prometheus î‚° k describe service/prometheus-prometheus-node-exporter
Name:              prometheus-prometheus-node-exporter
Namespace:         default
Labels:            app.kubernetes.io/component=metrics
                   app.kubernetes.io/instance=prometheus
                   app.kubernetes.io/managed-by=Helm
                   app.kubernetes.io/name=prometheus-node-exporter
                   app.kubernetes.io/part-of=prometheus-node-exporter
                   app.kubernetes.io/version=1.3.1
                   helm.sh/chart=prometheus-node-exporter-4.5.2
Annotations:       meta.helm.sh/release-name: prometheus
                   meta.helm.sh/release-namespace: default
                   prometheus.io/scrape: true
Selector:          app.kubernetes.io/instance=prometheus,app.kubernetes.io/name=prometheus-node-exporter
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.97.224.120
IPs:               10.97.224.120
Port:              metrics  9100/TCP
TargetPort:        9100/TCP
Endpoints:         10.0.3.139:9100,10.0.3.205:9100,10.0.4.192:9100 + 1 more...
Session Affinity:  None

```



https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config ë¥¼ ë³´ë©° ì˜ë¯¸ë¥¼ íŒŒì•…í•´ë³´ì.

![image](https://user-images.githubusercontent.com/28949162/204711794-f25d1fc7-3593-4578-8713-8f722457445b.png)

https?

configmapì—ì„œ schem: https -> scheme: http ë¡œ ë³€ê²½ í›„ ì¬ë°°í¬

![image](https://user-images.githubusercontent.com/28949162/204717839-c2b3e687-8f40-41af-b1dd-11421ce3c7b4.png)

kubernetes-node ì™€ kubernetes-node-cAdvisor ê¹Œì§€ ë©ë‹¬ì•„ ì£½ì—ˆë‹¤! ğŸ˜ŠğŸ˜ŠğŸ˜Š

ì›ìƒë³µêµ¬ í›„ ì¬ë°°í¬ í•˜ì.

![image](https://user-images.githubusercontent.com/28949162/204718365-994d549d-778d-4653-9197-1f45908e5ab7.png)

ì›ìƒë³µêµ¬í•˜ë‹ˆ ìŸ¤ë“¤ì€ ì‚´ì•˜ë‹¤. ì´ë²ˆ ì‹œë„ëŠ” ì‹¤íŒ¨


### check all component logs

`k logs kube-controller-manager-ip-10-0-3-139.ap-northeast-2.compute.internal -n kube-system`

```shell
I1130 02:55:05.854543       1 event.go:291] "Event occurred" object="default/prometheus-server" kind="Service" apiVersion="v1" type="Normal" reason="EnsuringLoadBalancer" message="Ensuring load balancer"
W1130 02:55:05.948809       1 endpointslice_controller.go:305] Error syncing endpoint slices for service "default/prometheus-server", retrying. Error: EndpointSlice informer cache is out of date
```
```
W1130 02:55:07.544453       1 aws.go:3239] Error authorizing security group ingress "InvalidPermission.Duplicate: the specified rule \"peer: 10.0.2.0/24, TCP, from port: 31027, to port: 31027, ALLOW\" already exists\n\tstatus code: 400, request id: 0197821f-7e34-4ae9-8629-d479d328766e"
W1130 02:55:07.544757       1 aws_loadbalancer.go:901] Error add traffic permission to security group: "error authorizing security group ingress: \"InvalidPermission.Duplicate: the specified rule \\\"peer: 10.0.2.0/24, TCP, from port: 31027, to port: 31027, ALLOW\\\" already exists\\n\\tstatus code: 400, request id: 0197821f-7e34-4ae9-8629-d479d328766e\""
W1130 02:55:07.544962       1 aws.go:4039] Error opening ingress rules for the load balancer to the instances: "error authorizing security group ingress: \"InvalidPermission.Duplicate: the specified rule \\\"peer: 10.0.2.0/24, TCP, from port: 31027, to port: 31027, ALLOW\\\" already exists\\n\\tstatus code: 400, request id: 0197821f-7e34-4ae9-8629-d479d328766e\""
E1130 02:55:07.545123       1 controller.go:310] error processing service default/prometheus-server (will retry): failed to ensure load balancer: error authorizing security group ingress: "InvalidPermission.Duplicate: the specified rule \"peer: 10.0.2.0/24, TCP, from port: 31027, to port: 31027, ALLOW\" already exists\n\tstatus code: 400, request id: 0197821f-7e34-4ae9-8629-d479d328766e"
I1130 02:55:07.545660       1 event.go:291] "Event occurred" object="default/prometheus-server" kind="Service" apiVersion="v1" type="Warning" reason="SyncLoadBalancerFailed" message="Error syncing load balancer: failed to ensure load balancer: error authorizing security group ingress: \"InvalidPermission.Duplicate: the specified rule \\\"peer: 10.0.2.0/24, TCP, from port: 31027, to port: 31027, ALLOW\\\" already exists\\n\\tstatus code: 400, request id: 0197821f-7e34-4ae9-8629-d479d328766e\""
I1130 02:55:07.545679       1 event.go:291] "Event occurred" object="default/prometheus-alertmanager" kind="Service" apiVersion="v1" type="Normal" reason="EnsuringLoadBalancer" message="Ensuring load balancer"
I1130 02:55:07.751128       1 aws_loadbalancer.go:181] Creating load balancer for default/prometheus-alertmanager with name: a8600f466aecf42c0877e61b4f8c90b0
I1130 02:55:08.306416       1 aws_loadbalancer.go:574] Creating load balancer target group for default/prometheus-alertmanager with name: k8s-default-promethe-32499709c8
I1130 02:55:08.641235       1 aws_loadbalancer.go:546] Creating load balancer listener for default/prometheus-alertmanager
W1130 02:55:09.191197       1 aws.go:3239] Error authorizing security group ingress "InvalidPermission.Duplicate: the specified rule \"peer: 10.0.0.0/24, TCP, from port: 32594, to port: 32594, ALLOW\" already exists\n\tstatus code: 400, request id: 05f8fc07-00e6-4d4c-8cdd-ef5ae3e5acd7"
W1130 02:55:09.191240       1 aws_loadbalancer.go:901] Error add traffic permission to security group: "error authorizing security group ingress: \"InvalidPermission.Duplicate: the specified rule \\\"peer: 10.0.0.0/24, TCP, from port: 32594, to port: 32594, ALLOW\\\" already exists\\n\\tstatus code: 400, request id: 05f8fc07-00e6-4d4c-8cdd-ef5ae3e5acd7\""
W1130 02:55:09.191261       1 aws.go:4039] Error opening ingress rules for the load balancer to the instances: "error authorizing security group ingress: \"InvalidPermission.Duplicate: the specified rule \\\"peer: 10.0.0.0/24, TCP, from port: 32594, to port: 32594, ALLOW\\\" already exists\\n\\tstatus code: 400, request id: 05f8fc07-00e6-4d4c-8cdd-ef5ae3e5acd7\""
E1130 02:55:09.191293       1 controller.go:310] error processing service default/prometheus-alertmanager (will retry): failed to ensure load balancer: error authorizing security group ingress: "InvalidPermission.Duplicate: the specified rule \"peer: 10.0.0.0/24, TCP, from port: 32594, to port: 32594, ALLOW\" already exists\n\tstatus code: 400, request id: 05f8fc07-00e6-4d4c-8cdd-ef5ae3e5acd7"
I1130 02:55:09.191582       1 event.go:291] "Event occurred" object="default/prometheus-alertmanager" kind="Service" apiVersion="v1" type="Warning" reason="SyncLoadBalancerFailed" message="Error syncing load balancer: failed to ensure load balancer: error authorizing security group ingress: \"InvalidPermission.Duplicate: the specified rule \\\"peer: 10.0.0.0/24, TCP, from port: 32594, to port: 32594, ALLOW\\\" already exists\\n\\tstatus code: 400, request id: 05f8fc07-00e6-4d4c-8cdd-ef5ae3e5acd7\""
I1130 02:55:12.553172       1 event.go:291] "Event occurred" object="default/prometheus-server" kind="Service" apiVersion="v1" type="Normal" reason="EnsuringLoadBalancer" message="Ensuring load balancer"
I1130 02:55:13.156513       1 event.go:291] "Event occurred" object="default/prometheus-server" kind="Service" apiVersion="v1" type="Normal" reason="EnsuredLoadBalancer" message="Ensured load balancer"
I1130 02:55:14.193757       1 event.go:291] "Event occurred" object="default/prometheus-alertmanager" kind="Service" apiVersion="v1" type="Normal" reason="EnsuringLoadBalancer" message="Ensuring load balancer"
I1130 02:55:14.558405       1 event.go:291] "Event occurred" object="default/prometheus-alertmanager" kind="Service" apiVersion="v1" type="Normal" reason="EnsuredLoadBalancer" message="Ensured load balancer"

```






### kubernetes api server í™•ì¸

`k logs  pod/kube-apiserver-ip-10-0-3-139.ap-northeast-2.compute.internal -n kube-system`

E1130 02:53:05.831201       1 watch.go:251] unable to encode watch object *v1.WatchEvent: write tcp 10.0.3.139:6443->10.0.3.205:12990: write: broken pipe (&streaming.encoder{writer:(*framer.lengthDelimitedFrameWriter)(0xc00da0c4e0), encoder:(*versioning.codec)(0xc010cc7cc0), buf:(*bytes.Buffer)(0xc00c9b58c0)})

10.0.3.139:6443 ì€ master node ì˜ kube-api server ì´ë‹¤.
```
 âš¡ root@ip-10-0-3-139 î‚° ~/prometheus î‚° k describe pod/kube-apiserver-ip-10-0-3-139.ap-northeast-2.compute.internal -n kube-system
Name:                 kube-apiserver-ip-10-0-3-139.ap-northeast-2.compute.internal
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 ip-10-0-3-139.ap-northeast-2.compute.internal/10.0.3.139
Start Time:           Tue, 29 Nov 2022 04:43:56 +0000
Labels:               component=kube-apiserver
                      tier=control-plane
Annotations:          kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.0.3.139:6443
```


# grafana ë¹„ë°€ë²ˆí˜¸ ì˜¤ë¥˜

helm re install ì‹œ
--set adminPassword="admin" í•´ì¤˜ë„ ë¹„ë°€ë²ˆí˜¸ ì—ëŸ¬ê°€ ëœ¸ 

![](https://velog.velcdn.com/images/hyunshoon/post/5ce14fa2-35e9-4134-a6f9-42780af16636/image.png)

ì´ì „ì— ì„¤ì¹˜í–ˆì„ ë•Œ ë¹„ë°€ë²ˆí˜¸ë¥¼ ë³€ê²½í•´ì£¼ì—ˆëŠ”ë°, helm ìœ¼ë¡œ ì¬ì„¤ì¹˜í•˜ë©´ ë¹„ë°€ë²ˆí˜¸ê°€ ì´ˆê¸°í™” ë˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ì´ì „ì— ë³€ê²½í•œ ë¹„ë°€ë²ˆí˜¸ë¡œ ì €ì¥ë˜ì–´ìˆìŒ.

# grafana readiness probe failed

![](https://velog.velcdn.com/images/hyunshoon/post/d9d9c449-408b-4e80-8a12-f7e900979292/image.png)

ë©°ì¹  ë’¤ ì¬ë°°í¬ë¥¼ í•´ë´¤ëŠ”ë° ê·¸ë¼íŒŒë‚˜ ëŒ€ì‹œë³´ë“œ ì—°ê²°ë§Œ ë˜ê³  ì†ŒìŠ¤ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•˜ëŠ” ë¬¸ì œê°€ ìˆë‹¤.

```shell
  Normal   Started    9m43s                  kubelet            Started container grafana
  Warning  Unhealthy  9m42s (x2 over 9m43s)  kubelet            Readiness probe failed: Get "http://192.168.161.59:3000/api/health": dial tcp 192.168.161.59:3000: connect: connection refused

```

ë°”ë€ í™˜ê²½ì´ ë­ê°€ìˆì„ê¹Œ ìƒê°í•´ë³´ë©´ ë©°ì¹ ì‚¬ì´ ê·¸ë¼íŒŒë‚˜ í—¬ë¦„ ì°¨íŠ¸ ë²„ì „ ì—…ê·¸ë ˆì´ë“œê°€ ìˆë‹¤.

![](https://velog.velcdn.com/images/hyunshoon/post/8dd02290-07f8-4984-915f-0ae366151a7d/image.png)

ë”°ë¼ì„œ, ê¸°ì¡´ì— --set ìœ¼ë¡œ overwrite í•˜ëŠ”ê²ƒì„ values.yaml íŒŒì¼ì„ ê³ ì³ì“°ëŠ” ë°©ë²•ìœ¼ë¡œ ë°”ê¿”ë³¸ë‹¤.

ë°”ê¾¼ values.yaml íŒŒì¼ì€ github ì— ì²¨ë¶€

```shell
helm install grafana grafana/grafana \
--set persistence.enabled=true \
--set persistence.existingClaim=grafana \
--set service.type=LoadBalancer \ #c
--set securityContext.runAsUser=1000 \ #c
--set securityContext.runAsGroup=1000 \ #c
--set adminPassword="admin" \ #check
```

![](https://velog.velcdn.com/images/hyunshoon/post/f405567e-4bff-4c1b-a2ab-098e28f51896/image.png)


```
helm install grafana ./grafana
```

í•˜ì§€ë§Œ, ì—¬ì „íˆ readinessProbe error ê°€ ëœ¨ë©° ë°ì´í„°ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í•œë‹¤.



## Ouch!
![](https://velog.velcdn.com/images/hyunshoon/post/f13395c8-c5b6-45ee-9ac8-c9c60dd26dd3/image.png)

ì ˆì•½ì„ ìœ„í•´ ê°œë°œí•˜ì§€ ì•Šì„ ë•Œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë‚´ë¦¬ê³ , í”„ë¡œë©”í…Œìš°ìŠ¤ë„ ë‚´ë¦¬ê¸° ë•Œë¬¸ì— ë‹¤ì‹œ ì˜¬ë¦´ ë•Œ Cluster IP ê°€ ë°”ë€ë‹¤ëŠ”ê±¸ ê¹Œë¨¹ì§€ ë§ì. ìœ„ IP ë¥¼ í˜„ì¬ í”„ë¡œë©”í…Œìš°ìŠ¤ IPë¡œ ë°”ê¿”ì£¼ë©´ ëœë‹¤.

ë²„ì „ íƒ“ ë¶€í„° í•˜ëŠ” ë‚´ ì‚¬ê³ ë°©ì‹ì„ ì† ë³¼ í•„ìš”ê°€ ìˆë‹¤ ğŸ¤·â€â™‚ï¸ 

# webhook trouble - Config reload

ì„ì˜ë¡œ ë…¸ë“œë¥¼ ë‹¤ìš´ì‹œì¼œë´¤ëŠ”ë°, prometheus serverì—ì„œëŠ” ì–¼ëŸ¿ì´ ì˜ ê°€ëŠ”ë° ìŠ¬ë™ìœ¼ë¡œ ì›¹í›…ì€ ë˜ ì•ˆëœë‹¤. ğŸ¤¦â€â™‚ï¸

ë˜ ì›¹í›… urlì´ ì´ì „ì— ì„¤ì •í•œ ê²ƒê³¼ ë‹¬ë¼ì ¸ìˆë‹¤. ë°”ë€ URLì„ ë³€ê²½í•´ì•¼í•œë‹¤.

í”„ë¡œë©”í…Œìš°ìŠ¤ ì„¤ì •ì„ ë°”ê¿€ ë•Œ prometheusë¥¼ ì¬ì„¤ì¹˜ í•˜ëŠ” ë°©ë²•ë§ê³  ë‹¤ë¥¸ ë°©ë„ê°€ ìˆì„ ê²ƒì´ë‹¤.

ê³µì‹ë¬¸ì„œì— ë”°ë¥´ë©´ í”„ë¡œë©”í…Œìš°ìŠ¤ëŠ” ëŸ°íƒ€ì„ì¤‘ì— ì„¤ì •ì„ reload í•  ìˆ˜ ìˆë‹¤. 
>
ê³µì‹ë¬¸ì„œ ë‚´ìš©
>
Prometheus can reload its configuration at runtime. If the new configuration is not well-formed, the changes will not be applied. A configuration reload is triggered by sending a SIGHUP to the Prometheus process or sending a HTTP POST request to the /-/reload endpoint (when the --web.enable-lifecycle flag is enabled). This will also reload any configured rule files.


ê³µì‹ë¬¸ì„œì— ë”°ë¥´ë©´ í”„ë¡œë©”í…Œìš°ìŠ¤ëŠ” ëŸ°íƒ€ì„ì¤‘ì— ì„¤ì •ì„ reload í•  ìˆ˜ ìˆë‹¤. 
>
Prometheus can reload its configuration at runtime. If the new configuration is not well-formed, the changes will not be applied. A configuration reload is triggered by sending a SIGHUP to the Prometheus process or sending a HTTP POST request to the /-/reload endpoint (when the --web.enable-lifecycle flag is enabled). This will also reload any configured rule files.


## How?

í”„ë¡œë©”í…Œìš°ìŠ¤ ì»¨í”¼ê·¸ë§µì„ ìˆ˜ì •í›„ ì¬ë°°í¬í•˜ë©´ ëœë‹¤.

```
kubectl get configmap ## configmap name í™•ì¸
kubectl get configmap -o yaml prometheus-alertmanager > prometheus_config.yaml
vi prometheus_config.yaml ### ìˆ˜ì •í•  ë¶€ë¶„ ìˆ˜ì •
```

![](https://velog.velcdn.com/images/hyunshoon/post/3df11406-3095-4f48-ac60-bc712c3b40aa/image.png)

ê¸°ì¡´ ì»¨í”¼ê·¸ë§µì´ë‹¤. 

receivers.slack_configs.textë¥¼ ë³€ê²½ í›„ ì¬ë°°í¬ í•´ë³¸ë‹¤.

ë³€ê²½ í›„ apply
```shell
âš¡ root@ip-10-0-3-181 î‚° ~/prometheus î‚° k apply -f prometheus_config.yml
configmap/prometheus-alertmanager configured
```
## Test
configmap describë¡œ í™•ì¸
```yaml
 âš¡ root@ip-10-0-3-181 î‚° ~/prometheus î‚° k describe configmap prometheus-alertmanager
Name:         prometheus-alertmanager
Namespace:    default
Labels:       app=prometheus
              app.kubernetes.io/managed-by=Helm
              chart=prometheus-16.0.0
              component=alertmanager
              heritage=Helm
              release=prometheus
Annotations:  meta.helm.sh/release-name: prometheus
              meta.helm.sh/release-namespace: default

Data
====
allow-snippet-annotations:
----
false
alertmanager.yml:
----
global:
  slack_api_url: https://hooks.slack.com/services/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
receivers:
- name: slack-notifier
  slack_configs:
  - channel: '#k8s-monitoring'
    send_resolved: true
    text: For reload Test.Node has been down for more than 1 minute.
route:
  group_interval: 1m
  group_wait: 10s
  receiver: slack-notifier
  repeat_interval: 2m

Events:  <none>
```

logsë¡œ í™•ì¸
```shell
 âš¡ root@ip-10-0-3-181 î‚° ~/prometheus î‚° k logs prometheus-alertmanager-585bf69d6d-pmvwg -c prometheus-alertmanager-configmap-reload
2022/11/27 04:25:02 Watching directory: "/etc/config"
2022/11/27 05:19:15 config map updated
2022/11/27 05:19:15 performing webhook request (1/1)
2022/11/27 05:19:15 successfully triggered reload
```
## webhook test
![](https://velog.velcdn.com/images/hyunshoon/post/c9fbf26b-0554-454e-b647-78c2a21c1f3e/image.png)

ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì§€í•˜ì—¬ ìŠ¬ë™ ì•ŒëŒ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³¸ë‹¤.

![](https://velog.velcdn.com/images/hyunshoon/post/1a0e25d3-f136-4175-9eeb-bf276bdaf7ac/image.png)

Reference
- https://prometheus.io/docs/prometheus/latest/configuration/configuration/


