apiVersion: v1
data:
  alerting_rules.yml: |
    groups:
    - name: AllInstances
      rules:
      - alert: InstanceScaleOut
        annotations:
          description: Cluster has  been scale out 1 node for more than 10 seconds.
          summary: Instance {{ $labels.instance }} scale out
        expr: (sum(kube_node_info) > sum(kube_node_info offset 1m))
        for: 10s
        labels:
          severity: "info"
      - alert: ImbalancedZoneA
        annotations:
          description:  The number of nodes in zoneA is less than 20% of the number of nodes in all availability zones.
          summary: ZoneA nodes imbalancing
        expr: sum(up{topology_kubernetes_io_zone=~'ap-northeast-3a',job="kubernetes-nodes"})  <= sum(up{job="kubernetes-nodes"}) * 0.2    
        for: 1m
        labels:
          severity: "info"
      - alert: ImbalancedZoneB
        annotations:
          description: The number of nodes in zoneB is less than 20% of the number of nodes in all availability zones.
          summary: ZoneB nodes imbalancing
        expr: sum(up{topology_kubernetes_io_zone=~'ap-northeast-3b',job="kubernetes-nodes"})  <= sum(up{job="kubernetes-nodes"}) * 0.2 
        for: 1m
        labels:
          severity: "info"
      - alert: ImbalancedZoneC
        annotations:
          description: The number of nodes in zoneC is less than 20% of the number of nodes in all availability zones.
          summary: ZoneC nodes imbalancing
        expr: sum(up{topology_kubernetes_io_zone=~'ap-northeast-3c',job="kubernetes-nodes"})  <= sum(up{job="kubernetes-nodes"}) * 0.2    
        for: 1m
        labels:
          severity: "info"
      - alert: MasterDown
        annotations:
          description: Master node {{ $labels.instance }} has been down more than 1 minute. You have to fix master node for quorom.
          summary: Master {{ $labels.instance }} down
        expr: up{key="master"} and up{job="kubernetes-nodes"} == 0    
        for: 1m
        labels:
          severity: "page"
      - alert: MoreThanHalfOfWorkers
        annotations:
          description: Instance Scaled Out 
          summary: AutoScaler capacity exceeds 50%
        expr: sum(up{job="kubernetes-nodes", key!="master"})  > (12 * 0.5)       
        for: 1m
        labels:
          severity: "info"
      - alert: MaxInstanceScaleOut
        annotations:
          description: You should increase the maximum scale out value or check the part where the problems occurred.  
          summary: AutoScaler capacity reached 100% 
        expr: sum(up{job="kubernetes-nodes", key!="master"})  == 12       
        for: 30s
        labels:
          severity: "page"
      - alert: PodCrashLoopBackOff
        annotations:
          description: 'Pod {{ $labels.pod }} is crash looping VALUE = {{ $value }} LABELS = {{ $labels }}'
          summary: Kubernetes pod crash looping (instance {{ $labels.instance }})
        expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
        for: 2m
        labels:
          severity: "info"
      - alert: ZoneCrashed
        annotations:
          description: 'Crashed Zone: {{ $labels.topology_kubernetes_io_zone }}'
          summary: ZoneFailed
        expr: sum by (topology_kubernetes_io_zone)(up{topology_kubernetes_io_zone=~"ap-northeast-3a"}) == 0 or sum by (topology_kubernetes_io_zone)(up{topology_kubernetes_io_zone=~"ap-northeast-3b"}) == 0 or sum by (topology_kubernetes_io_zone)(up{topology_kubernetes_io_zone=~"ap-northeast-3c"}) == 0 
        for: 0s
        labels:
          severity: "page"
      - alert: ScaleOutLimitation
        annotations:
          description: 'AWSAutoScaleGroup reached limit node count and unable to scale out any pod in all node.'
          summary: AutoScaler reached limitation 
        expr: sum (kube_pod_status_phase{phase="Pending", namespace="default", pod=~"flask.*"}) > 0 and sum(up{job="kubernetes-nodes", key!="master"}) == 12 
        for: 30s
        labels:
          severity: "critical"
      - alert: IncreasedHttpRequest
        annotations:
          description:  Http requests more than 3 times in 10 minutes
          summary: 3X Http requests 
        expr: sum_over_time(flask_http_request_total{status="200", job!="my-flask-app"}[10m]) * 3 < sum_over_time(flask_http_request_total{status="200", job!="my-flask-app"}[10m] offset 10m)
        for: 1m
        labels:
          severity: "info"
      - alert: UnusualHttpRequest
        annotations:
          description:  HTTP requests increased more than 10 times in 10 minutes
          summary: 10X Http requests 
        expr: sum_over_time(flask_http_request_total{status="200", job!="my-flask-app"}[10m]) * 3 < sum_over_time(flask_http_request_total{status="200", job!="my-flask-app"}[10m] offset 10m)
        for: 1m
        labels:
          severity: "page"
 
